{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw29NSYmFpyS"
   },
   "source": [
    "# Lab01: Linear Regression.\n",
    "\n",
    "- Student ID: 18127198\n",
    "- Student name: Lê Quang Bảo Sơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHR1Zj5GFpyT"
   },
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress folder in `zip` format and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REHkv-y8FpyU"
   },
   "source": [
    "### 1. The hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e75OMY0KFpyU"
   },
   "source": [
    "- Linear regression is a **linear** model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n",
    "- Generally, a linear model will make predictions by calculating a weighted sum of the input features (independent variables). \n",
    "$$ \\hat{y}=w_0+w_1x_1+w_2x_2+...+w_nx_n $$\n",
    "    - $\\hat{y}$ is the predicted value.\n",
    "    - $n$ is the number of features.\n",
    "    - $x_i$ is the $i^{th}$ feature value.\n",
    "    - $w_j$ is the $j^{th}$ model parameter (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n)$.\n",
    "$$\\hat{y}=h_{\\mathbf{w}}\\left(\\mathbf{x}\\right)=\\mathbf{w}^{T}\\cdot\\mathbf{x}$$\n",
    "    - $\\mathbf{w}$ is the model **parameter vector** (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n$).\n",
    "    - $\\mathbf{w}^T$ is a transpose  of $\\mathbf{w}$ (a row vector insteade of column vector).\n",
    "    - $\\mathbf{x}$ is the instance's **feature vector**, *containing* $x_0$ to $x_n$, with $x_0$ *always equal to* 1.\n",
    "    - $\\mathbf{w}^{T}\\cdot\\mathbf{x}$ is the dot product of $\\mathbf{w}^T$ and $\\mathbf{x}$.\n",
    "    - $h_{\\mathbf{w}}$ is the hypothesis function, using the parameters $\\mathbf{w}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l8F4lnjFpyV"
   },
   "source": [
    "### 2. Performance measure and the learning goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdJNZ2q6FpyX"
   },
   "source": [
    "- Before we start to train the model, we need to determine how good the model fits the training data. There are a couple of ways to determine the level of quality, but we are going to use the most popular one and that is the **MSE** (Mean Square Error). We need to find the value for $\\mathbf{w}$ that will minimize the MSE:\n",
    "$$\\mathbf{w}=\\arg\\min MSE_{\\mathcal{D}_{train}}$$\n",
    "\n",
    "\n",
    "- MSE on the train set $\\mathcal{D}_{train}$ denoted as $\\left(\\mathbf{X},\\mathbf{y}\\right)$ including m samples $\\left\\{\\left(\\mathbf{x}_1,y_1\\right),\\left(\\mathbf{x}_2,y_2\\right),...\\left(\\mathbf{x}_m,y_m\\right)\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTOZj7HfFpyY"
   },
   "source": [
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\sum_{i=1}^{m}\\left(\\mathbf{w}^T\\cdot\\mathbf{x}_i - y_i\\right )^2$$\n",
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\Vert\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example below is a plot of an MSE function where the true target value is 100, and the predicted values range between -10,000 to 10,000. The MSE loss (Y-axis) reaches its minimum value at prediction (X-axis) = 100.\n",
    "![Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)](MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORU-9tCxFpyZ"
   },
   "source": [
    "- To find the value of $\\mathbf{w}$ that minimizes the MSE cost function, the most common way (*we have known since high school*) is to solve the derivative (gradient) equation. \n",
    "$$\\mathbf{\\hat{w}}=\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}  \\mathbf{X}^T  \\mathbf{y}$$\n",
    "  - $\\mathbf{\\hat{w}}$ is the value of $\\mathbf{w}$ that minimizes the cost function\n",
    "  - **Notice that** $\\mathbf{X}^T  \\mathbf{X}$ is not always invertible. $\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}$ is pseudo-inverse of $\\left(\\mathbf{X}^T \\mathbf{X}\\right)$ - a general case of inverse when the matrix is not invertible or not even square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Tgy-tRYFpyZ"
   },
   "source": [
    "### 3. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qauCdk7LFpya"
   },
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70Mis-p9Fpyd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.datasets as datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRr06hARFpyk"
   },
   "source": [
    "#### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0K3G_SOFpyk"
   },
   "outputs": [],
   "source": [
    "X,y=datasets.make_regression(n_samples=100,n_features=1, noise=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBFWzeY3Fpyp"
   },
   "source": [
    "#### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BpxLtG3Fpyq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data \n",
    "\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLDAEVR8Fpyx"
   },
   "source": [
    "**TODO:** \n",
    "\n",
    "- Your observation about data: Data points are distributed quite evenly across the diagonal => high probability to find a suitable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrb7peM1Fpyz"
   },
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdPXTgoAFpyz"
   },
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    '''\n",
    "    Trains Linear Regression on the dataset (X, y).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (m, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    y : numpy array, shape (m, 1)\n",
    "        The vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (d + 1, 1)\n",
    "        The vector of parameters of Linear Regression after training.\n",
    "    '''\n",
    "    # TODO\n",
    "    XT=np.transpose(X)\n",
    "    XT_x=np.linalg.inv(XT@X)\n",
    "    Xt_x1=XT_x@XT\n",
    "    w=Xt_x1@y\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDgQ-5EDFpy5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_added_X.shape = (100, 2)\n",
      "y.shape = (100,)\n"
     ]
    }
   ],
   "source": [
    "# Construct one_added_X \n",
    "# TODO:\n",
    "# First column of one_added_X is all ones (corresponding to x_0).\n",
    "one=np.ones((100,1))\n",
    "one_added_X=np.hstack((one,X))\n",
    "print ('one_added_X.shape =', one_added_X.shape)\n",
    "print ('y.shape =', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVhd2dvCFpzE"
   },
   "source": [
    "#### Train our model and visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3YvmkEEFpzE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xt_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-db80d71f6c88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_linear_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_added_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Visualize result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredicted_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_added_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9cb6a17512f6>\u001b[0m in \u001b[0;36mtrain_linear_regression\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mXT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mXT_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXT\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mXt_x1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXt_x\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mXT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXt_x1\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xt_x' is not defined"
     ]
    }
   ],
   "source": [
    "w = train_linear_regression(one_added_X, y)\n",
    "\n",
    "# Visualize result\n",
    "predicted_ys = one_added_X.dot(w)\n",
    "\n",
    "plt.plot(X,y,'ro')\n",
    "\n",
    "x_min, x_max = plt.xlim()\n",
    "xs = np.array([x_min, x_max]).reshape(-1, 1)\n",
    "\n",
    "# Construct one_added_xs \n",
    "# TODO:\n",
    "# First column of one_added_xs is all ones (corresponding to x_0).\n",
    "ones_added_xs=np.hstack((np.ones((2,1)),xs))\n",
    "\n",
    "predicted_ys = ones_added_xs.dot(w)\n",
    "plt.plot(xs, predicted_ys)\n",
    "plt.xlim(x_min, x_max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTO6ilruFpzH"
   },
   "source": [
    "- **TODO**: Discuss about advantages and disadvantages of `Linear Regression`:\n",
    "\n",
    "-- advantages:\n",
    "+ easy to implement\n",
    "+ easier to interpret the output coefficients\n",
    "+ great when the relationship to between covariates and response variable is known to be linear.\n",
    "+ over simplifies many real world problems.\n",
    "\n",
    "-- disadvantages:\n",
    "+ sensitive to noise\n",
    "+ cannot represent complex models\n",
    "+ not a complete description of relationships among variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BehaTobaFpzI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01-LinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
